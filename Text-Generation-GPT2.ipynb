{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykksw5DE1v1L",
        "outputId": "ab38c657-a3a3-4d79-9042-e7141151d1c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "with pdfplumber.open(\"/content/ML.pdf\") as pdf:\n",
        "    text = \"\"\n",
        "    for page in pdf.pages:\n",
        "        text += page.extract_text()\n"
      ],
      "metadata": {
        "id": "ICA37lRM12Qx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "qa_pairs = re.findall(r\"(\\d+ .*?\\?)\\n(.*?)(?=\\n\\d+|\\Z)\", text, re.DOTALL)\n",
        "\n",
        "# Construire un dataset structuré\n",
        "dataset = [{\"prompt\": q.strip(), \"response\": a.strip()} for q, a in qa_pairs]\n",
        "\n",
        "# Sauvegarder en JSON\n",
        "with open(\"qa_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Afficher un aperçu\n",
        "#for pair in dataset:\n",
        "    #print(f\"Question: {pair['prompt']}\")\n",
        "    #print(f\"Réponse: {pair['response']}\\n\")"
      ],
      "metadata": {
        "id": "BhtWq4s417vG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Charger le fichier JSON\n",
        "with open('qa_dataset.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Vérifier le contenu\n",
        "print(data[20:22])  # Affiche les 2 premiers éléments\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLHJ0KrR2EPz",
        "outputId": "8be60936-0823-4060-ff98-49da2a431027"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'prompt': '29 Define non-negative matrix factorization. Give an example of its\\napplication.\\nMatrix factorization means factorizing a matrix into 2 or more matrices\\nsuch that the product of these matrices approximates the actual matrix. This\\ntechnique can greatly simplify the complex matrix operations and can be\\nused to find the latent features in the given data, such as in a\\nRecommendation system, where it could be used to find the similarities\\nbetween two users.\\nIn non-negative matrix factorization (NMF), a matrix is factorized into 2\\nsub-matrices such that all the 3 matrices have no negative elements. Apart\\nfrom Recommendation system, NMF can be applied to text mining, to\\nquery from a set of documents.\\nIn this technique, a document-term matrix is constructed from the input\\ngroup of documents and then factorized into term-feature and feature-\\ndocument matrices. It is also known as inverted indexing since it representsthe frequency of terms in each document and indexes each term which\\nmaps to the set of documents in which it is present, with the corresponding\\nfrequency. All the entries in these matrices, i.e. the frequency of the terms\\nand the index of the documents, are non-negative.\\n30 How is k-Nearest Neighbors (k-NN) different from k-Means\\nalgorithm?', 'response': 'A. The fundamental difference between these algorithms is that k-NN is\\na Supervised algorithm whereas k-means is Unsupervised in nature.\\nB. k-NN is a Classification (or Regression) algorithm and k-means is a\\nClustering algorithm.\\nC. k-NN tries to classify an observation based on its \"k\" surrounding\\nneighbors. It is also known as a lazy learner because it does absolutely\\nnothing at the training stage. On the other hand, k-means algorithm\\npartitions the training data set into different clusters such that all the\\ndata points in a cluster are closer to each other than the data points\\nfrom other clusters. The algorithm tries to maintain enough\\nseparability between these clusters.'}, {'prompt': '31 How do you select the important features in the given dataset?', 'response': 'Feature selection is a major step in the Machine Learning pipeline. What\\nyou learn from the data and how good is it depends on how efficiently the\\nselected features represent your dataset. We will discuss more about Feature\\nSelection in detail in the chapter on Data Preprocessing.\\nSome of the ways to select the important features from the dataset are:\\nA. Remove the correlated features prior to selecting the relevant features\\nbecause the correlated features make their significance stronger in the\\ntraining while leaving or reducing the importance of other features,\\nresulting in the model not being able to capture other features\\ncorrectly.\\nB. Use linear regression and select the variables based on their p-values.\\nP-value is the level of marginal significance which represents theprobability of the occurrence of a given event under the null\\nhypothesis, which, in our case, means that the feature is not important.\\nSo, for the features with small p-values (generally <= 0.05), you can\\nreject the null hypothesis and mark that feature as important.\\nC. Iteratively update each feature by either Forward Selection, Backward\\nElimination or Stepwise Selection technique.\\nD. Use Information Gain (the amount of information gained by knowing\\nthe value of a feature) to select the top \"k\" important features.\\nObviously, the features with the higher Information Gain are more\\nimportant and would be selected for the training.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Charger les données\n",
        "with open('qa_dataset.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Créer un fichier texte pour le fine-tuning\n",
        "with open('qa_finetuning.txt', 'w', encoding='utf-8') as file:\n",
        "    for qa in data:\n",
        "        file.write(f\"Question: {qa['prompt']}\\n\")\n",
        "        file.write(f\"Answer: {qa['response']}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "jNPeBH263bMs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Charger le modèle et le tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Charger les données\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "def create_data_collator(tokenizer):\n",
        "    return DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # Pour GPT, pas de Masked Language Modeling\n",
        "    )\n",
        "\n",
        "dataset = load_dataset(\"qa_finetuning.txt\", tokenizer)\n",
        "data_collator = create_data_collator(tokenizer)\n",
        "\n",
        "# Configurer l'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "# Sauvegarder le modèle finetuné\n",
        "model.save_pretrained(\"./gpt2_finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2_finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "4MC20icP3iNt",
        "outputId": "2be9c412-c28b-4aa2-fddc-f84564d5f810"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250103_230946-c0stht41</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumaimaezafa12-enset-mohammedia/huggingface/runs/c0stht41' target=\"_blank\">./gpt2_finetuned</a></strong> to <a href='https://wandb.ai/oumaimaezafa12-enset-mohammedia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/oumaimaezafa12-enset-mohammedia/huggingface' target=\"_blank\">https://wandb.ai/oumaimaezafa12-enset-mohammedia/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/oumaimaezafa12-enset-mohammedia/huggingface/runs/c0stht41' target=\"_blank\">https://wandb.ai/oumaimaezafa12-enset-mohammedia/huggingface/runs/c0stht41</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='324' max='324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [324/324 01:03, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2_finetuned/tokenizer_config.json',\n",
              " './gpt2_finetuned/special_tokens_map.json',\n",
              " './gpt2_finetuned/vocab.json',\n",
              " './gpt2_finetuned/merges.txt',\n",
              " './gpt2_finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Charger le modèle fine-tuné\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_finetuned\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_finetuned\")\n",
        "\n",
        "# Ajouter le token de padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tester avec une question\n",
        "input_text = \"Question: What is backpropagation in neural networks? Provide a concise explanation.\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Créer l'attention mask (1 pour les tokens non-padding, 0 pour les tokens padding)\n",
        "attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
        "\n",
        "# Générer une réponse\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,  # Passer l'attention mask ici\n",
        "    max_length=100,  # Augmenter max_length pour plus de contenu\n",
        "    num_return_sequences=1,\n",
        "    temperature=1.0,  # Augmenter la température pour plus de diversité\n",
        "    top_p=0.9,\n",
        "    no_repeat_ngram_size=2  # Empêcher la répétition des bigrammes\n",
        ")\n",
        "\n",
        "# Décoder et afficher la réponse\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "oQVFXya54ArP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe72d6b5-8f92-4745-c13c-71cfdc1c48e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is backpropagation in neural networks? Provide a concise explanation.\n",
            "Answer: Backpropagging is a technique used to train a neural network to perform a task. Backward training is when the training dataset is not completely accurate, but the model is still performing the task correctly. It is used for training a model to predict the outcome of the given task, such as the number of times the person is present in the present time. The goal is to minimize the error in predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Charger le modèle et le tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_finetuned\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_finetuned\")\n",
        "\n",
        "# Exemple de texte pour l'évaluation\n",
        "input_text = \"Question:What is backpropagation?\"\n",
        "\n",
        "# Tokenisation du texte d'entrée\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Calcul de la perplexité\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, labels=input_ids)\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hM_lglWJcS2",
        "outputId": "e28f1cb4-6db1-4f94-f25c-66149c209ebe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 19.07032585144043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZN0YlrBNtTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}